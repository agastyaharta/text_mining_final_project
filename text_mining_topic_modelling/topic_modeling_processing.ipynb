{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d5aaa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/agastyaharta/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/agastyaharta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/agastyaharta/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18eb6c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 .txt files\n",
      " - Crime_and_Punishment.txt\n",
      " - Notes_from_the_Underground.txt\n",
      " - Poor_Folk.txt\n",
      " - Short_Stories.txt\n",
      " - The_Brothers_Karamazov.txt\n",
      " - The_Gambler.txt\n",
      " - The_Grand_Inquisitor.txt\n",
      " - The_Idiot.txt\n",
      " - The_Possessed _or_The_Devils.txt\n",
      " - White_Nights_and_Other_Stories.txt\n",
      "Loaded books: ['Crime_and_Punishment', 'Notes_from_the_Underground', 'Poor_Folk', 'Short_Stories', 'The_Brothers_Karamazov', 'The_Gambler', 'The_Grand_Inquisitor', 'The_Idiot', 'The_Possessed _or_The_Devils', 'White_Nights_and_Other_Stories']\n"
     ]
    }
   ],
   "source": [
    "raw_dir = Path(\"data_raw\")\n",
    "raw_files = sorted(raw_dir.glob(\"*.txt\"))\n",
    "\n",
    "print(f\"Found {len(raw_files)} .txt files\")\n",
    "for f in raw_files[:10]:\n",
    "    print(\" -\", f.name)\n",
    "\n",
    "def read_text(fp: Path) -> str:\n",
    "    try:\n",
    "        return fp.read_text(encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        return fp.read_text(encoding=\"latin-1\")\n",
    "\n",
    "raw_texts = {fp.stem: read_text(fp) for fp in raw_files}\n",
    "print(\"Loaded books:\", list(raw_texts.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9d72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = pd.Series({k: len(v) for k, v in raw_texts.items()}).sort_values(ascending=False)\n",
    "print(lengths)\n",
    "\n",
    "print(\"\\nShortest book:\", lengths.index[-1], lengths.iloc[-1])\n",
    "print(\"Longest book:\", lengths.index[0], lengths.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11247e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime_and_Punishment -> CRIME AND PUNISHMENT PART I CHAPTER I On an exceptionally hot evening early in July a young man came out of the garret in which he lodged in S. Place and walked slowly, as though in hesitation, towards K. bridge. He had successfully avoided meeting his landlady on the staircase. His garret was under\n"
     ]
    }
   ],
   "source": [
    "def normalise_text_base(text: str) -> str:\n",
    "    text = text.replace(\"\\r\", \"\\n\")\n",
    "    text = re.sub(r\"[^\\x09\\x0A\\x0D\\x20-\\x7E]\", \" \", text)  # strip non-printing\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "books_base = {book_id: normalise_text_base(txt) for book_id, txt in raw_texts.items()}\n",
    "\n",
    "sample_book = next(iter(books_base))\n",
    "print(sample_book, \"->\", books_base[sample_book][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d2d22d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk-documents: (2395, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>0</td>\n",
       "      <td>Crime_and_Punishment__0000</td>\n",
       "      <td>CRIME AND PUNISHMENT PART I CHAPTER I On an ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>1</td>\n",
       "      <td>Crime_and_Punishment__0001</td>\n",
       "      <td>of mind; he walked along not observing what wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>2</td>\n",
       "      <td>Crime_and_Punishment__0002</td>\n",
       "      <td>the other into the street. This house was let ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>3</td>\n",
       "      <td>Crime_and_Punishment__0003</td>\n",
       "      <td>paused, as though hesitating; then stepped on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>4</td>\n",
       "      <td>Crime_and_Punishment__0004</td>\n",
       "      <td>he reflected. So she carries the keys in a poc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2390</th>\n",
       "      <td>White_Nights_and_Other_Stories</td>\n",
       "      <td>192</td>\n",
       "      <td>White_Nights_and_Other_Stories__0192</td>\n",
       "      <td>man had actually gone off his head, was utterl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2391</th>\n",
       "      <td>White_Nights_and_Other_Stories</td>\n",
       "      <td>193</td>\n",
       "      <td>White_Nights_and_Other_Stories__0193</td>\n",
       "      <td>must, while completely unconscious, have dragg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2392</th>\n",
       "      <td>White_Nights_and_Other_Stories</td>\n",
       "      <td>194</td>\n",
       "      <td>White_Nights_and_Other_Stories__0194</td>\n",
       "      <td>back, called up Okeanov, asked for the key of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2393</th>\n",
       "      <td>White_Nights_and_Other_Stories</td>\n",
       "      <td>195</td>\n",
       "      <td>White_Nights_and_Other_Stories__0195</td>\n",
       "      <td>bed it at once aroused suspicion, and some of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2394</th>\n",
       "      <td>White_Nights_and_Other_Stories</td>\n",
       "      <td>196</td>\n",
       "      <td>White_Nights_and_Other_Stories__0196</td>\n",
       "      <td>beyond his means. The landlady wailed without ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2395 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             book_id  chunk_index  \\\n",
       "0               Crime_and_Punishment            0   \n",
       "1               Crime_and_Punishment            1   \n",
       "2               Crime_and_Punishment            2   \n",
       "3               Crime_and_Punishment            3   \n",
       "4               Crime_and_Punishment            4   \n",
       "...                              ...          ...   \n",
       "2390  White_Nights_and_Other_Stories          192   \n",
       "2391  White_Nights_and_Other_Stories          193   \n",
       "2392  White_Nights_and_Other_Stories          194   \n",
       "2393  White_Nights_and_Other_Stories          195   \n",
       "2394  White_Nights_and_Other_Stories          196   \n",
       "\n",
       "                                  chunk_id  \\\n",
       "0               Crime_and_Punishment__0000   \n",
       "1               Crime_and_Punishment__0001   \n",
       "2               Crime_and_Punishment__0002   \n",
       "3               Crime_and_Punishment__0003   \n",
       "4               Crime_and_Punishment__0004   \n",
       "...                                    ...   \n",
       "2390  White_Nights_and_Other_Stories__0192   \n",
       "2391  White_Nights_and_Other_Stories__0193   \n",
       "2392  White_Nights_and_Other_Stories__0194   \n",
       "2393  White_Nights_and_Other_Stories__0195   \n",
       "2394  White_Nights_and_Other_Stories__0196   \n",
       "\n",
       "                                              text_base  \n",
       "0     CRIME AND PUNISHMENT PART I CHAPTER I On an ex...  \n",
       "1     of mind; he walked along not observing what wa...  \n",
       "2     the other into the street. This house was let ...  \n",
       "3     paused, as though hesitating; then stepped on ...  \n",
       "4     he reflected. So she carries the keys in a poc...  \n",
       "...                                                 ...  \n",
       "2390  man had actually gone off his head, was utterl...  \n",
       "2391  must, while completely unconscious, have dragg...  \n",
       "2392  back, called up Okeanov, asked for the key of ...  \n",
       "2393  bed it at once aroused suspicion, and some of ...  \n",
       "2394  beyond his means. The landlady wailed without ...  \n",
       "\n",
       "[2395 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_by_words(text: str, chunk_words: int = 800, overlap_words: int = 200,\n",
    "                   min_tail_words: int = 300) -> List[str]:\n",
    "    assert 0 <= overlap_words < chunk_words\n",
    "    words = text.split()\n",
    "    step = chunk_words - overlap_words\n",
    "\n",
    "    chunks = []\n",
    "    for start in range(0, len(words), step):\n",
    "        window = words[start:start + chunk_words]\n",
    "        if len(window) < min_tail_words:\n",
    "            break\n",
    "        chunks.append(\" \".join(window))\n",
    "    return chunks\n",
    "\n",
    "docs = []\n",
    "for book_id, text in books_base.items():\n",
    "    chunks = chunk_by_words(text, chunk_words=800, overlap_words=200, min_tail_words=300)\n",
    "    for i, ch in enumerate(chunks):\n",
    "        docs.append({\n",
    "            \"book_id\": book_id,\n",
    "            \"chunk_index\": i,\n",
    "            \"chunk_id\": f\"{book_id}__{i:04d}\",\n",
    "            \"text_base\": ch\n",
    "        })\n",
    "\n",
    "df_docs = pd.DataFrame(docs)\n",
    "print(\"chunk-documents:\", df_docs.shape)\n",
    "df_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df6541",
   "metadata": {},
   "source": [
    "Inspecting chunks of documents per books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e4d2593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_id\n",
      "The_Brothers_Karamazov            600\n",
      "The_Possessed _or_The_Devils      423\n",
      "The_Idiot                         412\n",
      "Crime_and_Punishment              345\n",
      "White_Nights_and_Other_Stories    197\n",
      "Short_Stories                     135\n",
      "The_Gambler                       102\n",
      "Poor_Folk                          91\n",
      "Notes_from_the_Underground         74\n",
      "The_Grand_Inquisitor               16\n",
      "Name: chunk_id, dtype: int64\n",
      "\n",
      "Total chunks: 2395\n"
     ]
    }
   ],
   "source": [
    "counts = df_docs.groupby(\"book_id\")[\"chunk_id\"].count().sort_values(ascending=False)\n",
    "print(counts)\n",
    "print(\"\\nTotal chunks:\", len(df_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7799c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "my_stopwords = set(stopwords.words(\"english\"))\n",
    "my_stopwords |= {\n",
    "    \"said\", \"say\", \"tell\",\n",
    "    \"would\", \"could\", \"must\", \"shall\",\n",
    "    \"one\",\n",
    "    \"though\", \"even\", \"well\", \"like\"\n",
    "}\n",
    "\n",
    "def _to_wordnet_pos(tag: str):\n",
    "    # penn tag -> wordnet pos\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    if tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    if tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    if tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "def preprocess_for_lda(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in my_stopwords and len(t) > 2]\n",
    "\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    tokens = [lemmatiser.lemmatize(t, pos=_to_wordnet_pos(tag)) for t, tag in tags]\n",
    "\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99d81e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_bert(text: str) -> str:\n",
    "    text = re.sub(r\"_+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a65ebeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped empty lda chunks: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text_base</th>\n",
       "      <th>text_lda</th>\n",
       "      <th>text_bert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>0</td>\n",
       "      <td>Crime_and_Punishment__0000</td>\n",
       "      <td>CRIME AND PUNISHMENT PART I CHAPTER I On an ex...</td>\n",
       "      <td>crime punishment part chapter exceptionally ho...</td>\n",
       "      <td>crime and punishment part i chapter i on an ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>1</td>\n",
       "      <td>Crime_and_Punishment__0001</td>\n",
       "      <td>of mind; he walked along not observing what wa...</td>\n",
       "      <td>mind walk along observe care observe time time...</td>\n",
       "      <td>of mind; he walked along not observing what wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>2</td>\n",
       "      <td>Crime_and_Punishment__0002</td>\n",
       "      <td>the other into the street. This house was let ...</td>\n",
       "      <td>street house let tiny tenement inhabit work pe...</td>\n",
       "      <td>the other into the street. this house was let ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>3</td>\n",
       "      <td>Crime_and_Punishment__0003</td>\n",
       "      <td>paused, as though hesitating; then stepped on ...</td>\n",
       "      <td>pause hesitating step side point door room let...</td>\n",
       "      <td>paused, as though hesitating; then stepped on ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>4</td>\n",
       "      <td>Crime_and_Punishment__0004</td>\n",
       "      <td>he reflected. So she carries the keys in a poc...</td>\n",
       "      <td>reflect carry key pocket right bunch steel rin...</td>\n",
       "      <td>he reflected. so she carries the keys in a poc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>5</td>\n",
       "      <td>Crime_and_Punishment__0005</td>\n",
       "      <td>drank off the first glassful. At once he felt ...</td>\n",
       "      <td>drank first glassful felt easier thought becom...</td>\n",
       "      <td>drank off the first glassful. at once he felt ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>6</td>\n",
       "      <td>Crime_and_Punishment__0006</td>\n",
       "      <td>an atmosphere might well make a man drunk. The...</td>\n",
       "      <td>atmosphere might make man drunk chance meeting...</td>\n",
       "      <td>an atmosphere might well make a man drunk. the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                book_id  chunk_index                    chunk_id  \\\n",
       "0  Crime_and_Punishment            0  Crime_and_Punishment__0000   \n",
       "1  Crime_and_Punishment            1  Crime_and_Punishment__0001   \n",
       "2  Crime_and_Punishment            2  Crime_and_Punishment__0002   \n",
       "3  Crime_and_Punishment            3  Crime_and_Punishment__0003   \n",
       "4  Crime_and_Punishment            4  Crime_and_Punishment__0004   \n",
       "5  Crime_and_Punishment            5  Crime_and_Punishment__0005   \n",
       "6  Crime_and_Punishment            6  Crime_and_Punishment__0006   \n",
       "\n",
       "                                           text_base  \\\n",
       "0  CRIME AND PUNISHMENT PART I CHAPTER I On an ex...   \n",
       "1  of mind; he walked along not observing what wa...   \n",
       "2  the other into the street. This house was let ...   \n",
       "3  paused, as though hesitating; then stepped on ...   \n",
       "4  he reflected. So she carries the keys in a poc...   \n",
       "5  drank off the first glassful. At once he felt ...   \n",
       "6  an atmosphere might well make a man drunk. The...   \n",
       "\n",
       "                                            text_lda  \\\n",
       "0  crime punishment part chapter exceptionally ho...   \n",
       "1  mind walk along observe care observe time time...   \n",
       "2  street house let tiny tenement inhabit work pe...   \n",
       "3  pause hesitating step side point door room let...   \n",
       "4  reflect carry key pocket right bunch steel rin...   \n",
       "5  drank first glassful felt easier thought becom...   \n",
       "6  atmosphere might make man drunk chance meeting...   \n",
       "\n",
       "                                           text_bert  \n",
       "0  crime and punishment part i chapter i on an ex...  \n",
       "1  of mind; he walked along not observing what wa...  \n",
       "2  the other into the street. this house was let ...  \n",
       "3  paused, as though hesitating; then stepped on ...  \n",
       "4  he reflected. so she carries the keys in a poc...  \n",
       "5  drank off the first glassful. at once he felt ...  \n",
       "6  an atmosphere might well make a man drunk. the...  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_docs[\"text_lda\"] = df_docs[\"text_base\"].apply(preprocess_for_lda)\n",
    "df_docs[\"text_bert\"] = df_docs[\"text_base\"].apply(preprocess_for_bert)\n",
    "\n",
    "before = len(df_docs)\n",
    "df_docs = df_docs[df_docs[\"text_lda\"].str.len() > 0].reset_index(drop=True)\n",
    "print(\"dropped empty lda chunks:\", before - len(df_docs))\n",
    "\n",
    "#df_docs[[\"book_id\", \"chunk_id\", \"text_lda\", \"text_bert\"]]\n",
    "df_docs.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6091f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_id\n",
      "The_Brothers_Karamazov            600\n",
      "The_Possessed _or_The_Devils      423\n",
      "The_Idiot                         412\n",
      "Crime_and_Punishment              345\n",
      "White_Nights_and_Other_Stories    197\n",
      "Short_Stories                     135\n",
      "The_Gambler                       102\n",
      "Poor_Folk                          91\n",
      "Notes_from_the_Underground         74\n",
      "The_Grand_Inquisitor               16\n",
      "Name: chunk_id, dtype: int64\n",
      "\n",
      "Total chunks: 2395\n"
     ]
    }
   ],
   "source": [
    "counts = df_docs.groupby(\"book_id\")[\"chunk_id\"].count().sort_values(ascending=False)\n",
    "print(counts)\n",
    "print(\"\\nTotal chunks:\", len(df_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c4f4f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty lda chunks: 0\n",
      "\n",
      "example lda snippet:\n",
      " crime punishment part chapter exceptionally hot even early july young man come garret lodged place walk slowly hesitation towards bridge successfully avoided meeting landlady staircase garret roof high five storied house cupboard room landlady provide garret dinner attendance lived floor every time go obliged pas kitchen door invariably stand open time pass young man sick frighten feel make scowl \n",
      "\n",
      "example raw snippet:\n",
      " CRIME AND PUNISHMENT PART I CHAPTER I On an exceptionally hot evening early in July a young man came out of the garret in which he lodged in S. Place and walked slowly, as though in hesitation, towards K. bridge. He had successfully avoided meeting his landlady on the staircase. His garret was under the roof of a high, five-storied house and was more like a cupboard than a room. The landlady who p\n"
     ]
    }
   ],
   "source": [
    "print(\"empty lda chunks:\", (df_docs[\"text_lda\"].str.len() == 0).sum())\n",
    "print(\"\\nexample lda snippet:\\n\", df_docs[\"text_lda\"].iloc[0][:400])\n",
    "print(\"\\nexample raw snippet:\\n\", df_docs[\"text_base\"].iloc[0][:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0a6fbfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /Users/agastyaharta/Desktop/wne_uw/3/project/text_mining_final/text_mining_topic_modelling/data_cleaned/dostoevsky_chunks.csv\n",
      "rows: 2395 cols: 6\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path(\"data_cleaned\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "out_path = out_dir / \"dostoevsky_chunks.csv\"\n",
    "df_docs.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"saved:\", out_path.resolve())\n",
    "print(\"rows:\", len(df_docs), \"cols:\", df_docs.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a1fa6",
   "metadata": {},
   "source": [
    "### PREVIOUS WORKS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c739305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prelim_clean(text: str) -> str:\n",
    "    return re.sub(r\"[^a-zA-Z0-9 \\n\\.\\']\", '', text)\n",
    "\n",
    "prelim_texts = {}\n",
    "for book, text in texts.items():\n",
    "    prelim = prelim_clean(text)\n",
    "    prelim_texts[book] = prelim\n",
    "\n",
    "print(prelim_texts[first_key][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfcbf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(text: str) -> str:\n",
    "    return re.sub(r' +', ' ', text)\n",
    "\n",
    "def remove_punctuation_remaining(text: str) -> str:\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "def remove_numbers(text: str) -> str:\n",
    "    return re.sub(r'\\d', '', text)\n",
    "\n",
    "clean_texts = {}\n",
    "for book, text in prelim_texts.items():\n",
    "    t = remove_extra_spaces(text)\n",
    "    t = remove_punctuation_remaining(t)\n",
    "    t = remove_numbers(t)\n",
    "    clean_texts[book] = t\n",
    "\n",
    "print(clean_texts[first_key][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_texts = {book: text.lower() for book, text in clean_texts.items()}\n",
    "print(lower_texts[first_key][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1315adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text: str) -> list[str]:\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    tokens = word_tokenize(text)\n",
    "    return [w for w in tokens if w not in stop_words and w.strip() != \"\"]\n",
    "\n",
    "filtered_tokens = {}\n",
    "filtered_texts = {}\n",
    "\n",
    "for book, text in lower_texts.items():\n",
    "    tokens_kept = remove_stopwords(text)\n",
    "    filtered_tokens[book] = tokens_kept\n",
    "    filtered_texts[book] = \" \".join(tokens_kept)\n",
    "\n",
    "print(\"Original token sample:\", word_tokenize(lower_texts[first_key])[:40])\n",
    "print(\"After stopword removal:\", filtered_tokens[first_key][:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887bea8",
   "metadata": {},
   "source": [
    "maybe i'll do word count here and see what is the most recent word and see if i should remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cda5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tokens = []\n",
    "for tokens in filtered_tokens.values():\n",
    "    all_tokens.extend(tokens)\n",
    "\n",
    "freq = Counter(all_tokens)\n",
    "freq.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = {\n",
    "    \"said\",\"say\",\"tell\",\n",
    "    \"would\",\"could\",\"must\",\"shall\",\n",
    "    \"one\",\n",
    "    \"though\",\"even\",\"well\",\"like\",\n",
    "    \"come\",\"go\",\"went\",\"see\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf53038",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_tokens = {}\n",
    "stemmed_texts = {}\n",
    "\n",
    "for book, tokens in filtered_tokens.items(): \n",
    "    stems = [ps.stem(w) for w in tokens]\n",
    "    stemmed_tokens[book] = stems\n",
    "    stemmed_texts[book] = \" \".join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb814820",
   "metadata": {},
   "outputs": [],
   "source": [
    "BOOK_OUT = Path(\"data_cleaned\")\n",
    "BOOK_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for book, text in stemmed_texts.items():\n",
    "    (BOOK_OUT / f\"{book}_clean.txt\").write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Saved cleaned books to:\", BOOK_OUT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
