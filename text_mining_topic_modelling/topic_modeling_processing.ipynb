{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d5aaa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/agastyaharta/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/agastyaharta/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/agastyaharta/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18eb6c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 .txt files\n",
      " - Crime_and_Punishment.txt\n",
      " - Notes_from_the_Underground.txt\n",
      " - Poor_Folk.txt\n",
      " - Short_Stories.txt\n",
      " - The_Brothers_Karamazov.txt\n",
      " - The_Gambler.txt\n",
      " - The_Grand_Inquisitor.txt\n",
      " - The_Idiot.txt\n",
      " - The_Possessed _or_The_Devils.txt\n",
      " - White_Nights_and_Other_Stories.txt\n",
      "Loaded books: ['Crime_and_Punishment', 'Notes_from_the_Underground', 'Poor_Folk', 'Short_Stories', 'The_Brothers_Karamazov', 'The_Gambler', 'The_Grand_Inquisitor', 'The_Idiot', 'The_Possessed _or_The_Devils', 'White_Nights_and_Other_Stories']\n"
     ]
    }
   ],
   "source": [
    "raw_dir = Path(\"data_raw\")\n",
    "raw_files = sorted(raw_dir.glob(\"*.txt\"))\n",
    "\n",
    "print(f\"Found {len(raw_files)} .txt files\")\n",
    "for f in raw_files[:10]:\n",
    "    print(\" -\", f.name)\n",
    "\n",
    "def read_text(fp: Path) -> str:\n",
    "    try:\n",
    "        return fp.read_text(encoding=\"utf-8\")\n",
    "    except UnicodeDecodeError:\n",
    "        return fp.read_text(encoding=\"latin-1\")\n",
    "\n",
    "raw_texts = {fp.stem: read_text(fp) for fp in raw_files}\n",
    "print(\"Loaded books:\", list(raw_texts.keys())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d9d72a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The_Brothers_Karamazov            1930493\n",
      "The_Possessed _or_The_Devils      1433585\n",
      "The_Idiot                         1347584\n",
      "Crime_and_Punishment              1130549\n",
      "White_Nights_and_Other_Stories     647262\n",
      "Short_Stories                      446001\n",
      "The_Gambler                        331469\n",
      "Poor_Folk                          291758\n",
      "Notes_from_the_Underground         238698\n",
      "The_Grand_Inquisitor                53221\n",
      "dtype: int64\n",
      "\n",
      "Shortest book: The_Grand_Inquisitor 53221\n",
      "Longest book: The_Brothers_Karamazov 1930493\n"
     ]
    }
   ],
   "source": [
    "lengths = pd.Series({k: len(v) for k, v in raw_texts.items()}).sort_values(ascending=False)\n",
    "print(lengths)\n",
    "\n",
    "print(\"\\nShortest book:\", lengths.index[-1], lengths.iloc[-1])\n",
    "print(\"Longest book:\", lengths.index[0], lengths.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11247e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crime_and_Punishment -> CRIME AND PUNISHMENT PART I CHAPTER I On an exceptionally hot evening early in July a young man came out of the garret in which he lodged in S. Place and walked slowly, as though in hesitation, towards K. bridge. He had successfully avoided meeting his landlady on the staircase. His garret was under\n"
     ]
    }
   ],
   "source": [
    "def normalise_text_base(text: str) -> str:\n",
    "    text = text.replace(\"\\r\", \"\\n\")\n",
    "    text = re.sub(r\"[^\\x09\\x0A\\x0D\\x20-\\x7E]\", \" \", text)  # strip non-printing\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "books_base = {book_id: normalise_text_base(txt) for book_id, txt in raw_texts.items()}\n",
    "\n",
    "sample_book = next(iter(books_base))\n",
    "print(sample_book, \"->\", books_base[sample_book][:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d2d22d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk-documents: (4787, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text_base</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>0</td>\n",
       "      <td>Crime_and_Punishment__0000</td>\n",
       "      <td>CRIME AND PUNISHMENT PART I CHAPTER I On an ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>1</td>\n",
       "      <td>Crime_and_Punishment__0001</td>\n",
       "      <td>the street, he became acutely aware of his fea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>2</td>\n",
       "      <td>Crime_and_Punishment__0002</td>\n",
       "      <td>of mind; he walked along not observing what wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>3</td>\n",
       "      <td>Crime_and_Punishment__0003</td>\n",
       "      <td>one side in a most unseemly fashion. Not shame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>4</td>\n",
       "      <td>Crime_and_Punishment__0004</td>\n",
       "      <td>the other into the street. This house was let ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4782</th>\n",
       "      <td>White_Nights_and_Other_Stories</td>\n",
       "      <td>389</td>\n",
       "      <td>White_Nights_and_Other_Stories__0389</td>\n",
       "      <td>be amiss to cut up the whole mattress with sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4783</th>\n",
       "      <td>White_Nights_and_Other_Stories</td>\n",
       "      <td>390</td>\n",
       "      <td>White_Nights_and_Other_Stories__0390</td>\n",
       "      <td>bed it at once aroused suspicion, and some of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4784</th>\n",
       "      <td>White_Nights_and_Other_Stories</td>\n",
       "      <td>391</td>\n",
       "      <td>White_Nights_and_Other_Stories__0391</td>\n",
       "      <td>not a million, though it did turn out to be a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4785</th>\n",
       "      <td>White_Nights_and_Other_Stories</td>\n",
       "      <td>392</td>\n",
       "      <td>White_Nights_and_Other_Stories__0392</td>\n",
       "      <td>beyond his means. The landlady wailed without ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4786</th>\n",
       "      <td>White_Nights_and_Other_Stories</td>\n",
       "      <td>393</td>\n",
       "      <td>White_Nights_and_Other_Stories__0393</td>\n",
       "      <td>the fact that death tears away the veil from a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4787 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             book_id  chunk_index  \\\n",
       "0               Crime_and_Punishment            0   \n",
       "1               Crime_and_Punishment            1   \n",
       "2               Crime_and_Punishment            2   \n",
       "3               Crime_and_Punishment            3   \n",
       "4               Crime_and_Punishment            4   \n",
       "...                              ...          ...   \n",
       "4782  White_Nights_and_Other_Stories          389   \n",
       "4783  White_Nights_and_Other_Stories          390   \n",
       "4784  White_Nights_and_Other_Stories          391   \n",
       "4785  White_Nights_and_Other_Stories          392   \n",
       "4786  White_Nights_and_Other_Stories          393   \n",
       "\n",
       "                                  chunk_id  \\\n",
       "0               Crime_and_Punishment__0000   \n",
       "1               Crime_and_Punishment__0001   \n",
       "2               Crime_and_Punishment__0002   \n",
       "3               Crime_and_Punishment__0003   \n",
       "4               Crime_and_Punishment__0004   \n",
       "...                                    ...   \n",
       "4782  White_Nights_and_Other_Stories__0389   \n",
       "4783  White_Nights_and_Other_Stories__0390   \n",
       "4784  White_Nights_and_Other_Stories__0391   \n",
       "4785  White_Nights_and_Other_Stories__0392   \n",
       "4786  White_Nights_and_Other_Stories__0393   \n",
       "\n",
       "                                              text_base  \n",
       "0     CRIME AND PUNISHMENT PART I CHAPTER I On an ex...  \n",
       "1     the street, he became acutely aware of his fea...  \n",
       "2     of mind; he walked along not observing what wa...  \n",
       "3     one side in a most unseemly fashion. Not shame...  \n",
       "4     the other into the street. This house was let ...  \n",
       "...                                                 ...  \n",
       "4782  be amiss to cut up the whole mattress with sci...  \n",
       "4783  bed it at once aroused suspicion, and some of ...  \n",
       "4784  not a million, though it did turn out to be a ...  \n",
       "4785  beyond his means. The landlady wailed without ...  \n",
       "4786  the fact that death tears away the veil from a...  \n",
       "\n",
       "[4787 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chunk_by_words(text: str, chunk_words: int = 800, overlap_words: int = 200,\n",
    "                   min_tail_words: int = 300) -> List[str]:\n",
    "    assert 0 <= overlap_words < chunk_words\n",
    "    words = text.split()\n",
    "    step = chunk_words - overlap_words\n",
    "\n",
    "    chunks = []\n",
    "    for start in range(0, len(words), step):\n",
    "        window = words[start:start + chunk_words]\n",
    "        if len(window) < min_tail_words:\n",
    "            break\n",
    "        chunks.append(\" \".join(window))\n",
    "    return chunks\n",
    "\n",
    "docs = []\n",
    "for book_id, text in books_base.items():\n",
    "    chunks = chunk_by_words(text, chunk_words=350, overlap_words=50, min_tail_words=150)\n",
    "    for i, ch in enumerate(chunks):\n",
    "        docs.append({\n",
    "            \"book_id\": book_id,\n",
    "            \"chunk_index\": i,\n",
    "            \"chunk_id\": f\"{book_id}__{i:04d}\",\n",
    "            \"text_base\": ch\n",
    "        })\n",
    "\n",
    "df_docs = pd.DataFrame(docs)\n",
    "print(\"chunk-documents:\", df_docs.shape)\n",
    "df_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08df6541",
   "metadata": {},
   "source": [
    "Inspecting chunks of documents per books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e4d2593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_id\n",
      "The_Brothers_Karamazov            1201\n",
      "The_Possessed _or_The_Devils       846\n",
      "The_Idiot                          824\n",
      "Crime_and_Punishment               689\n",
      "White_Nights_and_Other_Stories     394\n",
      "Short_Stories                      269\n",
      "The_Gambler                        204\n",
      "Poor_Folk                          181\n",
      "Notes_from_the_Underground         148\n",
      "The_Grand_Inquisitor                31\n",
      "Name: chunk_id, dtype: int64\n",
      "\n",
      "Total chunks: 4787\n"
     ]
    }
   ],
   "source": [
    "counts = df_docs.groupby(\"book_id\")[\"chunk_id\"].count().sort_values(ascending=False)\n",
    "print(counts)\n",
    "print(\"\\nTotal chunks:\", len(df_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7799c4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "my_stopwords = set(stopwords.words(\"english\"))\n",
    "my_stopwords |= {\n",
    "    \"said\", \"say\", \"tell\",\n",
    "    \"would\", \"could\", \"must\", \"shall\",\n",
    "    \"one\",\n",
    "    \"though\", \"even\", \"well\", \"like\", \"know\", \"come\", \"look\", \"see\", \"go\", \"make\", \"man\", \"time\",\n",
    "    \n",
    "    # Crime and Punishment\n",
    "    \"rodion\", \"romanovich\", \"raskolnikov\", \"sofya\", \"semyonovna\", \"marmeladova\",\n",
    "    \"sonya\", \"sonia\", \"avdotya\", \"dunya\", \"dmitri\", \"prokofych\", \"razumikhin\",\n",
    "    \"porfiry\", \"petrovich\", \"pyotr\", \"luzhin\", \"arkady\", \"ivanovich\",\n",
    "    \"svidrigailov\", \"pulcheria\", \"alexandrovna\", \"semyon\", \"zakharovich\",\n",
    "    \"marmeladov\", \"katerina\", \"ivanovna\", \"alyona\", \"lizaveta\",\n",
    "\n",
    "    # Notes from Underground\n",
    "    \"liza\", \"zverkov\", \"simonov\", \"ferfichkin\", \"trudolyubov\", \"apollon\",\n",
    "    \"anton\", \"antonich\", \"syetochkin\",\n",
    "\n",
    "    # Poor Folk\n",
    "    \"makar\", \"alekseyevich\", \"devushkin\", \"varvara\", \"dobroselova\", \"bykov\",\n",
    "\n",
    "    # The Gambler\n",
    "    \"alexei\", \"general\", \"sagoryanski\", \"polina\", \"des\", \"grieux\", \"astley\",\n",
    "    \"blanche\", \"cominges\", \"antonida\", \"vasilevna\", \"tarasevitcheva\",\n",
    "    \"maria\", \"filippovna\", \"wurmerhelm\", \"potapyts\", \"marfa\",\n",
    "\n",
    "    # The Idiot\n",
    "    \"lev\", \"nikolayevich\", \"myshkin\", \"anastassya\", \"nastassya\", \"filippovna\",\n",
    "    \"barashkov\", \"parfyon\", \"semyonovich\", \"rogozhin\", \"aglaya\", \"yepanchin\",\n",
    "    \"gavril\", \"ganya\", \"ardalionovich\", \"ivolgin\", \"ivan\", \"fyodorovich\",\n",
    "    \"lizaveta\", \"prokofyevna\", \"adelaida\", \"alexandra\", \"afanasy\", \"totsky\",\n",
    "    \"lebedev\", \"ippolit\", \"terentyev\", \"ardalion\", \"kolya\", \"nikolay\", \"varya\",\n",
    "\n",
    "    # Demons / The Devils\n",
    "    \"nikolai\", \"vsevolodovich\", \"stavrogin\", \"verkhovensky\",\n",
    "    \"stepanovich\", \"stepan\", \"trofimovich\", \"stavrogina\", \"pavlovich\",\n",
    "    \"shatov\", \"aleksey\", \"nilych\", \"kirillov\", \"lebyadkin\", \"marya\",\n",
    "    \"lebyadkina\",\n",
    "\n",
    "    # The Brothers Karamazov + Grand Inquisitor frame\n",
    "    \"fyodor\", \"pavlovich\", \"karamazov\", \"mitya\", \"pavel\", \"smerdyakov\",\n",
    "    \"agrafena\", \"svetlova\", \"grushenka\", \"verkhovtseva\", \"zosima\", \"grigory\",\n",
    "    \"vasilievich\", \"alyosha\",\n",
    "\n",
    "    # Grand Inquisitor parable tokens\n",
    "    \"christ\", \"jesus\", \"inquisitor\", \"seville\",\n",
    "\n",
    "    # White Nights\n",
    "    \"nastenka\", \"matryona\", \"fyokla\",\n",
    "\n",
    "    # Other stories mentioned (common editions)\n",
    "    \"vasya\", \"ivanovitch\", \"julian\", \"mastakovich\",\n",
    "    \"polzunkov\", \"fedosey\", \"nikolaitch\", \"osip\", \"mihalitch\",\n",
    "    \"prokharchin\",\n",
    "}\n",
    "\n",
    "def _to_wordnet_pos(tag: str):\n",
    "    # penn tag -> wordnet pos\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    if tag.startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    if tag.startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    if tag.startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "def preprocess_for_lda(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-z\\s]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "\n",
    "    tokens = [t for t in text.split() if len(t) > 2]\n",
    "\n",
    "    tags = nltk.pos_tag(tokens)\n",
    "    lemmas = [lemmatiser.lemmatize(t, pos=_to_wordnet_pos(tag)) for t, tag in tags]\n",
    "\n",
    "    # stopword removal AFTER lemmatisation (critical)\n",
    "    lemmas = [t for t in lemmas if t not in my_stopwords and len(t) > 2]\n",
    "\n",
    "    return \" \".join(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99d81e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_for_bert(text: str) -> str:\n",
    "    text = re.sub(r\"_+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a65ebeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped empty lda chunks: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>chunk_index</th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>text_base</th>\n",
       "      <th>text_lda</th>\n",
       "      <th>text_bert</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>0</td>\n",
       "      <td>Crime_and_Punishment__0000</td>\n",
       "      <td>CRIME AND PUNISHMENT PART I CHAPTER I On an ex...</td>\n",
       "      <td>crime punishment part chapter exceptionally ho...</td>\n",
       "      <td>crime and punishment part i chapter i on an ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>1</td>\n",
       "      <td>Crime_and_Punishment__0001</td>\n",
       "      <td>the street, he became acutely aware of his fea...</td>\n",
       "      <td>street become acutely aware fear want attempt ...</td>\n",
       "      <td>the street, he became acutely aware of his fea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>2</td>\n",
       "      <td>Crime_and_Punishment__0002</td>\n",
       "      <td>of mind; he walked along not observing what wa...</td>\n",
       "      <td>mind walk along observe care observe mutter so...</td>\n",
       "      <td>of mind; he walked along not observing what wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>3</td>\n",
       "      <td>Crime_and_Punishment__0003</td>\n",
       "      <td>one side in a most unseemly fashion. Not shame...</td>\n",
       "      <td>side unseemly fashion shame however quite anot...</td>\n",
       "      <td>one side in a most unseemly fashion. not shame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>4</td>\n",
       "      <td>Crime_and_Punishment__0004</td>\n",
       "      <td>the other into the street. This house was let ...</td>\n",
       "      <td>street house let tiny tenement inhabit work pe...</td>\n",
       "      <td>the other into the street. this house was let ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>5</td>\n",
       "      <td>Crime_and_Punishment__0005</td>\n",
       "      <td>clearly before him.... He started, his nerves ...</td>\n",
       "      <td>clearly start nerve terribly overstrain little...</td>\n",
       "      <td>clearly before him.... he started, his nerves ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Crime_and_Punishment</td>\n",
       "      <td>6</td>\n",
       "      <td>Crime_and_Punishment__0006</td>\n",
       "      <td>paused, as though hesitating; then stepped on ...</td>\n",
       "      <td>pause hesitate step side point door room let v...</td>\n",
       "      <td>paused, as though hesitating; then stepped on ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                book_id  chunk_index                    chunk_id  \\\n",
       "0  Crime_and_Punishment            0  Crime_and_Punishment__0000   \n",
       "1  Crime_and_Punishment            1  Crime_and_Punishment__0001   \n",
       "2  Crime_and_Punishment            2  Crime_and_Punishment__0002   \n",
       "3  Crime_and_Punishment            3  Crime_and_Punishment__0003   \n",
       "4  Crime_and_Punishment            4  Crime_and_Punishment__0004   \n",
       "5  Crime_and_Punishment            5  Crime_and_Punishment__0005   \n",
       "6  Crime_and_Punishment            6  Crime_and_Punishment__0006   \n",
       "\n",
       "                                           text_base  \\\n",
       "0  CRIME AND PUNISHMENT PART I CHAPTER I On an ex...   \n",
       "1  the street, he became acutely aware of his fea...   \n",
       "2  of mind; he walked along not observing what wa...   \n",
       "3  one side in a most unseemly fashion. Not shame...   \n",
       "4  the other into the street. This house was let ...   \n",
       "5  clearly before him.... He started, his nerves ...   \n",
       "6  paused, as though hesitating; then stepped on ...   \n",
       "\n",
       "                                            text_lda  \\\n",
       "0  crime punishment part chapter exceptionally ho...   \n",
       "1  street become acutely aware fear want attempt ...   \n",
       "2  mind walk along observe care observe mutter so...   \n",
       "3  side unseemly fashion shame however quite anot...   \n",
       "4  street house let tiny tenement inhabit work pe...   \n",
       "5  clearly start nerve terribly overstrain little...   \n",
       "6  pause hesitate step side point door room let v...   \n",
       "\n",
       "                                           text_bert  \n",
       "0  crime and punishment part i chapter i on an ex...  \n",
       "1  the street, he became acutely aware of his fea...  \n",
       "2  of mind; he walked along not observing what wa...  \n",
       "3  one side in a most unseemly fashion. not shame...  \n",
       "4  the other into the street. this house was let ...  \n",
       "5  clearly before him.... he started, his nerves ...  \n",
       "6  paused, as though hesitating; then stepped on ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_docs[\"text_lda\"] = df_docs[\"text_base\"].apply(preprocess_for_lda)\n",
    "df_docs[\"text_bert\"] = df_docs[\"text_base\"].apply(preprocess_for_bert)\n",
    "\n",
    "before = len(df_docs)\n",
    "df_docs = df_docs[df_docs[\"text_lda\"].str.len() > 0].reset_index(drop=True)\n",
    "print(\"dropped empty lda chunks:\", before - len(df_docs))\n",
    "\n",
    "#df_docs[[\"book_id\", \"chunk_id\", \"text_lda\", \"text_bert\"]]\n",
    "df_docs.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6091f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "book_id\n",
      "The_Brothers_Karamazov            1201\n",
      "The_Possessed _or_The_Devils       846\n",
      "The_Idiot                          824\n",
      "Crime_and_Punishment               689\n",
      "White_Nights_and_Other_Stories     394\n",
      "Short_Stories                      269\n",
      "The_Gambler                        204\n",
      "Poor_Folk                          181\n",
      "Notes_from_the_Underground         148\n",
      "The_Grand_Inquisitor                31\n",
      "Name: chunk_id, dtype: int64\n",
      "\n",
      "Total chunks: 4787\n"
     ]
    }
   ],
   "source": [
    "counts = df_docs.groupby(\"book_id\")[\"chunk_id\"].count().sort_values(ascending=False)\n",
    "print(counts)\n",
    "print(\"\\nTotal chunks:\", len(df_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4f4f885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "empty lda chunks: 0\n",
      "\n",
      "example lda snippet:\n",
      " crime punishment part chapter exceptionally hot early july young garret lodge place walk slowly hesitation towards bridge successfully avoid meet landlady staircase garret roof high five storied house cupboard room landlady provide garret dinner attendance live floor every obliged pas kitchen door invariably stand open pass young sick frighten feeling scowl feel ashamed hopelessly debt landlady af\n",
      "\n",
      "example raw snippet:\n",
      " CRIME AND PUNISHMENT PART I CHAPTER I On an exceptionally hot evening early in July a young man came out of the garret in which he lodged in S. Place and walked slowly, as though in hesitation, towards K. bridge. He had successfully avoided meeting his landlady on the staircase. His garret was under the roof of a high, five-storied house and was more like a cupboard than a room. The landlady who p\n"
     ]
    }
   ],
   "source": [
    "print(\"empty lda chunks:\", (df_docs[\"text_lda\"].str.len() == 0).sum())\n",
    "print(\"\\nexample lda snippet:\\n\", df_docs[\"text_lda\"].iloc[0][:400])\n",
    "print(\"\\nexample raw snippet:\\n\", df_docs[\"text_base\"].iloc[0][:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0a6fbfd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: /Users/agastyaharta/Desktop/wne_uw/3/project/text_mining_final/text_mining_topic_modelling/data_cleaned/dostoevsky_chunks.csv\n",
      "rows: 4787 cols: 6\n"
     ]
    }
   ],
   "source": [
    "out_dir = Path(\"data_cleaned\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "out_path = out_dir / \"dostoevsky_chunks.csv\"\n",
    "df_docs.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"saved:\", out_path.resolve())\n",
    "print(\"rows:\", len(df_docs), \"cols:\", df_docs.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7a1fa6",
   "metadata": {},
   "source": [
    "### PREVIOUS WORKS!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c739305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prelim_clean(text: str) -> str:\n",
    "#     return re.sub(r\"[^a-zA-Z0-9 \\n\\.\\']\", '', text)\n",
    "\n",
    "# prelim_texts = {}\n",
    "# for book, text in texts.items():\n",
    "#     prelim = prelim_clean(text)\n",
    "#     prelim_texts[book] = prelim\n",
    "\n",
    "# print(prelim_texts[first_key][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbfcbf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def remove_extra_spaces(text: str) -> str:\n",
    "#     return re.sub(r' +', ' ', text)\n",
    "\n",
    "# def remove_punctuation_remaining(text: str) -> str:\n",
    "#     return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "# def remove_numbers(text: str) -> str:\n",
    "#     return re.sub(r'\\d', '', text)\n",
    "\n",
    "# clean_texts = {}\n",
    "# for book, text in prelim_texts.items():\n",
    "#     t = remove_extra_spaces(text)\n",
    "#     t = remove_punctuation_remaining(t)\n",
    "#     t = remove_numbers(t)\n",
    "#     clean_texts[book] = t\n",
    "\n",
    "# print(clean_texts[first_key][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1c7364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower_texts = {book: text.lower() for book, text in clean_texts.items()}\n",
    "# print(lower_texts[first_key][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1315adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# def remove_stopwords(text: str) -> list[str]:\n",
    "#     text = re.sub(r'\\s+', ' ', text).strip()\n",
    "#     tokens = word_tokenize(text)\n",
    "#     return [w for w in tokens if w not in stop_words and w.strip() != \"\"]\n",
    "\n",
    "# filtered_tokens = {}\n",
    "# filtered_texts = {}\n",
    "\n",
    "# for book, text in lower_texts.items():\n",
    "#     tokens_kept = remove_stopwords(text)\n",
    "#     filtered_tokens[book] = tokens_kept\n",
    "#     filtered_texts[book] = \" \".join(tokens_kept)\n",
    "\n",
    "# print(\"Original token sample:\", word_tokenize(lower_texts[first_key])[:40])\n",
    "# print(\"After stopword removal:\", filtered_tokens[first_key][:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7887bea8",
   "metadata": {},
   "source": [
    "maybe i'll do word count here and see what is the most recent word and see if i should remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6cda5b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tokens = []\n",
    "# for tokens in filtered_tokens.values():\n",
    "#     all_tokens.extend(tokens)\n",
    "\n",
    "# freq = Counter(all_tokens)\n",
    "# freq.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "994e020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_stopwords = {\n",
    "#     \"said\",\"say\",\"tell\",\n",
    "#     \"would\",\"could\",\"must\",\"shall\",\n",
    "#     \"one\",\n",
    "#     \"though\",\"even\",\"well\",\"like\",\n",
    "#     \"come\",\"go\",\"went\",\"see\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebf53038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps = PorterStemmer()\n",
    "\n",
    "# stemmed_tokens = {}\n",
    "# stemmed_texts = {}\n",
    "\n",
    "# for book, tokens in filtered_tokens.items(): \n",
    "#     stems = [ps.stem(w) for w in tokens]\n",
    "#     stemmed_tokens[book] = stems\n",
    "#     stemmed_texts[book] = \" \".join(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eb814820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BOOK_OUT = Path(\"data_cleaned\")\n",
    "# BOOK_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# for book, text in stemmed_texts.items():\n",
    "#     (BOOK_OUT / f\"{book}_clean.txt\").write_text(text, encoding=\"utf-8\")\n",
    "\n",
    "# print(\"Saved cleaned books to:\", BOOK_OUT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
